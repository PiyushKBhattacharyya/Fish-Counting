{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Counting Algorithm using YOLOv8\n",
    "\n",
    "This notebook implements a fish counting algorithm using YOLOv8 object detection on sonar imagery from the Fish Counting dataset.\n",
    "\n",
    "## Dataset Overview\n",
    "- Images from various river regions (Kenai, Nushagak, Elwha, etc.)\n",
    "- Sonar imagery with fish detections\n",
    "- Different strata and sampling locations\n",
    "\n",
    "## Algorithm Steps\n",
    "1. Data exploration and preprocessing\n",
    "2. YOLOv8 model training\n",
    "3. Fish detection and counting\n",
    "4. Evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install ultralytics opencv-python pandas numpy matplotlib\n",
    "# !pip install jupyter\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "ROOT_DIR = Path(\"..\")\n",
    "DATA_DIR = ROOT_DIR / \"Data\" / \"tiny_dataset\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "METADATA_DIR = DATA_DIR / \"metadata-tiny\"\n",
    "\n",
    "print(f\"Root directory: {ROOT_DIR.absolute()}\")\n",
    "print(f\"Data directory: {DATA_DIR.absolute()}\")\n",
    "print(f\"Raw data directory: {RAW_DIR.absolute()}\")\n",
    "print(f\"Metadata directory: {METADATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's explore the dataset structure and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "def load_metadata():\n",
    "    metadata_files = list(METADATA_DIR.glob(\"*.json\"))\n",
    "    all_metadata = []\n",
    "    \n",
    "    for file_path in metadata_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                item['region'] = file_path.stem.split('-')[0]\n",
    "                all_metadata.append(item)\n",
    "    \n",
    "    return pd.DataFrame(all_metadata)\n",
    "\n",
    "metadata_df = load_metadata()\n",
    "print(f\"Loaded metadata for {len(metadata_df)} video clips\")\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total clips: {len(metadata_df)}\")\n",
    "print(f\"Regions: {metadata_df['region'].unique()}\")\n",
    "print(f\"Average frames per clip: {metadata_df['num_frames'].mean():.1f}\")\n",
    "print(f\"Average framerate: {metadata_df['framerate'].mean():.1f} fps\")\n",
    "print(f\"Image dimensions: {metadata_df[['width', 'height']].mode().iloc[0].to_dict()}\")\n",
    "\n",
    "# Plot distribution of clips by region\n",
    "plt.figure(figsize=(10, 6))\n",
    "metadata_df['region'].value_counts().plot(kind='bar')\n",
    "plt.title('Number of Clips by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Number of Clips')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Analysis\n",
    "\n",
    "Let's examine some sample images from different regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sample images\n",
    "def get_sample_images(region, n_samples=5):\n",
    "    region_clips = metadata_df[metadata_df['region'] == region]\n",
    "    if len(region_clips) == 0:\n",
    "        return []\n",
    "    \n",
    "    sample_clip = region_clips.sample(1).iloc[0]\n",
    "    clip_dir = RAW_DIR / region / sample_clip['clip_name']\n",
    "    \n",
    "    if not clip_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    image_files = list(clip_dir.glob(\"*.jpg\"))\n",
    "    if len(image_files) < n_samples:\n",
    "        n_samples = len(image_files)\n",
    "    \n",
    "    return np.random.choice(image_files, n_samples, replace=False)\n",
    "\n",
    "# Display sample images from different regions\n",
    "regions = metadata_df['region'].unique()\n",
    "fig, axes = plt.subplots(len(regions), 3, figsize=(15, 5*len(regions)))\n",
    "\n",
    "for i, region in enumerate(regions):\n",
    "    sample_images = get_sample_images(region, 3)\n",
    "    \n",
    "    for j, img_path in enumerate(sample_images):\n",
    "        if i < len(axes) and j < len(axes[i]):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                axes[i][j].imshow(img)\n",
    "                axes[i][j].set_title(f\"{region} - {img_path.name}\")\n",
    "                axes[i][j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Prepare the data for YOLO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YOLO dataset structure\n",
    "# Note: This assumes we have annotations or will create them\n",
    "# For now, we'll set up the basic structure\n",
    "\n",
    "YOLO_DATA_DIR = ROOT_DIR / \"yolo_data\"\n",
    "YOLO_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Create data.yaml for YOLO\n",
    "data_yaml = f\"\"\"\n",
    "train: {YOLO_DATA_DIR / 'images' / 'train'}\n",
    "val: {YOLO_DATA_DIR / 'images' / 'val'}\n",
    "\n",
    "nc: 1\n",
    "names: ['fish']\n",
    "\"\"\"\n",
    "\n",
    "with open(YOLO_DATA_DIR / \"data.yaml\", 'w') as f:\n",
    "    f.write(data_yaml)\n",
    "\n",
    "print(\"YOLO data structure created\")\n",
    "print(f\"Data YAML saved to: {YOLO_DATA_DIR / 'data.yaml'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. YOLO Model Setup and Training\n",
    "\n",
    "Configure and train the YOLOv8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Load pretrained model\n",
    "\n",
    "# Display model info\n",
    "print(model.info())\n",
    "\n",
    "# Note: Training would require labeled data\n",
    "# For demonstration, we'll show the training command\n",
    "\"\"\"\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=YOLO_DATA_DIR / 'data.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='fish_counting'\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fish Counting Implementation\n",
    "\n",
    "Implement the counting logic based on detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fish_in_image(image_path, model, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Count fish in a single image using YOLO detection.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image\n",
    "        model: Trained YOLO model\n",
    "        conf_threshold (float): Confidence threshold for detections\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detection results including count and bounding boxes\n",
    "    \"\"\"\n",
    "    # Run inference\n",
    "    results = model(image_path, conf=conf_threshold)\n",
    "    \n",
    "    # Extract detections\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            detection = {\n",
    "                'bbox': box.xyxy[0].cpu().numpy(),\n",
    "                'confidence': box.conf[0].cpu().numpy(),\n",
    "                'class': int(box.cls[0].cpu().numpy())\n",
    "            }\n",
    "            detections.append(detection)\n",
    "    \n",
    "    return {\n",
    "        'count': len(detections),\n",
    "        'detections': detections,\n",
    "        'image_path': image_path\n",
    "    }\n",
    "\n",
    "def count_fish_in_clip(clip_dir, model, conf_threshold=0.5, max_frames=None):\n",
    "    \"\"\"\n",
    "    Count fish in all frames of a video clip.\n",
    "    \n",
    "    Args:\n",
    "        clip_dir (Path): Directory containing clip images\n",
    "        model: Trained YOLO model\n",
    "        conf_threshold (float): Confidence threshold\n",
    "        max_frames (int): Maximum number of frames to process (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Aggregated counting results for the clip\n",
    "    \"\"\"\n",
    "    image_files = sorted(list(clip_dir.glob(\"*.jpg\")))\n",
    "    \n",
    "    if max_frames:\n",
    "        image_files = image_files[:max_frames]\n",
    "    \n",
    "    clip_results = []\n",
    "    total_count = 0\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=f\"Processing {clip_dir.name}\"):\n",
    "        result = count_fish_in_image(str(img_path), model, conf_threshold)\n",
    "        clip_results.append(result)\n",
    "        total_count += result['count']\n",
    "    \n",
    "    return {\n",
    "        'clip_name': clip_dir.name,\n",
    "        'total_frames': len(image_files),\n",
    "        'total_fish_count': total_count,\n",
    "        'avg_fish_per_frame': total_count / len(image_files) if image_files else 0,\n",
    "        'frame_results': clip_results\n",
    "    }\n",
    "\n",
    "print(\"Fish counting functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions\n",
    "\n",
    "Create functions to visualize detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections(image_path, detections, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize fish detections on an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image\n",
    "        detections (list): List of detection dictionaries\n",
    "        save_path (str): Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for detection in detections:\n",
    "        bbox = detection['bbox']\n",
    "        conf = detection['confidence']\n",
    "        \n",
    "        # Convert to int\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        \n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f\"Fish: {conf:.2f}\"\n",
    "        cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    # Add count text\n",
    "    count_text = f\"Fish Count: {len(detections)}\"\n",
    "    cv2.putText(img, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                1, (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Fish Detection Results - {Path(image_path).name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "def plot_counting_results(clip_results):\n",
    "    \"\"\"\n",
    "    Plot fish counting results over time.\n",
    "    \n",
    "    Args:\n",
    "        clip_results (dict): Results from count_fish_in_clip\n",
    "    \"\"\"\n",
    "    frame_counts = [frame['count'] for frame in clip_results['frame_results']]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(frame_counts, marker='o', linestyle='-')\n",
    "    plt.title(f\"Fish Count Over Time - {clip_results['clip_name']}\")\n",
    "    plt.xlabel('Frame Number')\n",
    "    plt.ylabel('Number of Fish Detected')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nClip: {clip_results['clip_name']}\")\n",
    "    print(f\"Total frames processed: {clip_results['total_frames']}\")\n",
    "    print(f\"Total fish detected: {clip_results['total_fish_count']}\")\n",
    "    print(f\"Average fish per frame: {clip_results['avg_fish_per_frame']:.2f}\")\n",
    "    print(f\"Max fish in single frame: {max(frame_counts)}\")\n",
    "\n",
    "print(\"Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "\n",
    "Add functions to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_counts, predicted_counts):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for fish counting.\n",
    "    \n",
    "    Args:\n",
    "        true_counts (list): Ground truth fish counts\n",
    "        predicted_counts (list): Predicted fish counts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    true_counts = np.array(true_counts)\n",
    "    predicted_counts = np.array(predicted_counts)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = np.mean(np.abs(true_counts - predicted_counts))\n",
    "    \n",
    "    # Root Mean Square Error\n",
    "    rmse = np.sqrt(np.mean((true_counts - predicted_counts)**2))\n",
    "    \n",
    "    # Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs((true_counts - predicted_counts) / (true_counts + 1e-8))) * 100\n",
    "    \n",
    "    # Accuracy within tolerance (±1 fish)\n",
    "    accuracy_1 = np.mean(np.abs(true_counts - predicted_counts) <= 1)\n",
    "    \n",
    "    # Accuracy within tolerance (±2 fish)\n",
    "    accuracy_2 = np.mean(np.abs(true_counts - predicted_counts) <= 2)\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'Accuracy_±1': accuracy_1,\n",
    "        'Accuracy_±2': accuracy_2\n",
    "    }\n",
    "\n",
    "def evaluate_model_on_clip(clip_results, ground_truth_counts=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on a clip.\n",
    "    \n",
    "    Args:\n",
    "        clip_results (dict): Results from count_fish_in_clip\n",
    "        ground_truth_counts (list): Ground truth counts for comparison\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    if ground_truth_counts is None:\n",
    "        # If no ground truth, just return basic statistics\n",
    "        predicted_counts = [frame['count'] for frame in clip_results['frame_results']]\n",
    "        return {\n",
    "            'mean_count': np.mean(predicted_counts),\n",
    "            'std_count': np.std(predicted_counts),\n",
    "            'min_count': min(predicted_counts),\n",
    "            'max_count': max(predicted_counts),\n",
    "            'total_frames': len(predicted_counts)\n",
    "        }\n",
    "    \n",
    "    predicted_counts = [frame['count'] for frame in clip_results['frame_results']]\n",
    "    metrics = calculate_metrics(ground_truth_counts, predicted_counts)\n",
    "    \n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'predicted_counts': predicted_counts,\n",
    "        'ground_truth_counts': ground_truth_counts\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing and Demonstration\n",
    "\n",
    "Test the algorithm on sample data. (Note: This would require a trained model and ground truth data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (would need trained model and data)\n",
    "\"\"\"\n",
    "# Select a sample clip for testing\n",
    "sample_clip = metadata_df.sample(1).iloc[0]\n",
    "clip_path = RAW_DIR / sample_clip['region'] / sample_clip['clip_name']\n",
    "\n",
    "print(f\"Testing on clip: {sample_clip['clip_name']}\")\n",
    "print(f\"Region: {sample_clip['region']}\")\n",
    "print(f\"Frames: {sample_clip['num_frames']}\")\n",
    "\n",
    "# Count fish in the clip (using first 10 frames for demo)\n",
    "results = count_fish_in_clip(clip_path, model, max_frames=10)\n",
    "\n",
    "# Visualize results\n",
    "plot_counting_results(results)\n",
    "\n",
    "# Show detection on a sample frame\n",
    "sample_image = list(clip_path.glob(\"*.jpg\"))[0]\n",
    "sample_result = count_fish_in_image(str(sample_image), model)\n",
    "visualize_detections(str(sample_image), sample_result['detections'])\n",
    "\n",
    "# Evaluate performance\n",
    "evaluation = evaluate_model_on_clip(results)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for key, value in evaluation.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing code prepared (commented out due to lack of trained model)\")\n",
    "print(\"To run testing:\")\n",
    "print(\"1. Train the YOLO model with annotated data\")\n",
    "print(\"2. Uncomment the testing code above\")\n",
    "print(\"3. Run the evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete framework for fish counting using YOLOv8 object detection:\n",
    "\n",
    "### Key Components:\n",
    "1. **Data Exploration**: Load and analyze dataset metadata and sample images\n",
    "2. **Preprocessing**: Prepare data for YOLO training\n",
    "3. **Model Training**: Configure and train YOLOv8 model\n",
    "4. **Fish Counting**: Implement detection-based counting logic\n",
    "5. **Visualization**: Display detection results and counting statistics\n",
    "6. **Evaluation**: Calculate performance metrics\n",
    "\n",
    "### Next Steps:\n",
    "- Annotate training data for YOLO\n",
    "- Train the model on fish detection\n",
    "- Fine-tune hyperparameters\n",
    "- Validate on test set\n",
    "- Deploy for real-time counting\n",
    "\n",
    "### Requirements:\n",
    "- PyTorch with CUDA support\n",
    "- Ultralytics YOLOv8\n",
    "- OpenCV for image processing\n",
    "- Pandas and NumPy for data handling\n",
    "- Matplotlib for visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}